---
title: "solutions to the exercises in ‚ÄúData analysis recipes: Fitting a model to data‚Äù"
subtitle: "Julia implementation of solutions to the exercises in David Hogg‚Äôs 2010 tutorial paper on fitting a model to data"
author:
  - name: "PTA"
    affiliations: "Distinguished Scholar of Treasure Hoarder Socio-Economics, Northland Bank archives, Snezhnaya"
license: "CC BY"
engine: julia
julia:
  exeflags: ["--threads=auto", "--project=."]
format:
  pdf:
    colorlinks: true
    highlight-style: vim-dark
    documentclass: scrreprt
    papersize: a4
    geometry: ["margin=1cm", "footskip=5mm"]
    include-in-header:
      text: |
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
    include-before-body:
      text: |
        \RecustomVerbatimEnvironment{verbatim}{Verbatim}{showspaces=false,showtabs=false,breaksymbolleft={},breaklines}
    monofont: "JuliaMono-Regular"
  html:
    page-layout: full
    toc-location: left-body
    theme:
      light: united
      dark: superhero
    respect-user-color-scheme: true
    highlight-style: gruvbox
    title-block-banner: true
    anchor-sections: false
    smooth-scroll: true
    code-fold: show
    code-copy: true
    include-in-header:
      text: |
        <style>
        @font-face {font-family: JuliaMono; src: url("https://cdn.jsdelivr.net/gh/cormullion/juliamono-webfonts/JuliaMono-Regular.woff2");}
        pre {font-family: JuliaMono, monospace;}
        </style>
    output-file: index.html
toc: true
number-sections: true
code-line-numbers: true
fig-align: center
callout-appearance: minimal
editor: source
---

<!--
use fvextra in latex to break long line in code block

to use local fonts, they must be installed at system level, e.g. C:\Windows\Fonts or /usr/local/share/fonts/

to show plot in pdf, need rsvg-convert
- in linux: install librsvg or the like
- in windows to show plot: download rsvg-convert.exe into <quarto path>\bin\tools
  - https://github.com/miyako/console-rsvg-convert/releases or
  - https://sourceforge.net/projects/tumagcc/files/rsvg-convert-2.40.20.7z/download
-->

# preliminary {.unnumbered}

source code of this document: <https://github.com/phineas-pta/hogg2010/blob/main/hogg2010.qmd>

rendered using `quarto`

## references & resources {.unnumbered}

reference: **David W. Hogg**, **Jo Bovy**, **Dustin Lang**. 2010. *Data analysis recipes: Fitting a model to data*

- paper: <https://arxiv.org/pdf/1008.4686>
- source code: <https://github.com/davidwhogg/DataAnalysisRecipes/blob/master/straightline/straightline.tex>
- lecture: <https://www.youtube.com/playlist?list=PLBB44462E5201DD1E>

additional resources: various Python implementations, but often not all exercises are solved

- <https://astrowizici.st/teaching/phs5000/>
- <https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-robust-with-outlier-detection.html>
- <https://github.com/binado/fitaline/blob/main/fitaline.ipynb>
- <https://github.com/wdconinc/data-analysis-recipes-fitting-a-model-to-data/blob/master/Exercises_in_%22Data_Analysis_Recipes_Fitting_a_Model_to_Data%22.ipynb>
- <https://github.com/nhuntwalker/data_analysis_recipes/blob/master/worked_exercises.ipynb>
- <https://github.com/jimbarrett27/hogg2010/blob/master/solutions.ipynb>
- <https://github.com/nadanai263/fittingmodeltodata/blob/master/Fitting%20a%20model%20to%20data%20(Hogg%2C%20Bovy%2C%20Lang%202010).ipynb>
- <https://github.com/Lachimax/hblfit/blob/main/notebooks/1-standard-practise.ipynb>

## data {.unnumbered}

data to be used throughout the tutorial:

| $x$ | $y$ | $\sigma_x$ | $\sigma_y$ | $\rho_{xy}$ |
|----:|----:|-----------:|-----------:|------------:|
| 201 | 592 |          9 |         61 |       -0.84 |
| 244 | 401 |          4 |         25 |        0.31 |
|  47 | 583 |         11 |         38 |        0.64 |
| 287 | 402 |          7 |         15 |       -0.27 |
| 203 | 495 |          5 |         21 |       -0.33 |
|  58 | 173 |          9 |         15 |        0.67 |
| 210 | 479 |          4 |         27 |       -0.02 |
| 202 | 504 |          4 |         14 |       -0.05 |
| 198 | 510 |         11 |         30 |       -0.84 |
| 158 | 416 |          7 |         16 |       -0.69 |
| 165 | 393 |          5 |         14 |        0.30 |
| 201 | 442 |          5 |         25 |       -0.46 |
| 157 | 317 |          5 |         52 |       -0.03 |
| 131 | 311 |          6 |         16 |        0.50 |
| 166 | 400 |          6 |         34 |        0.73 |
| 160 | 337 |          5 |         31 |       -0.52 |
| 186 | 423 |          9 |         42 |        0.90 |
| 125 | 334 |          8 |         26 |        0.40 |
| 218 | 533 |          6 |         16 |       -0.78 |
| 146 | 344 |          5 |         22 |       -0.56 |

The full uncertainty covariance matrix for each data point is given by $\begin{bmatrix} \sigma_x^2 & \sigma_{xy} \\ \sigma_{xy} & \sigma_y^2 \end{bmatrix}$ with $\sigma_{xy}=\rho_{xy}\sigma_x\sigma_y$

given the authors‚Äô academic background, those data have very likely astrophysics origin (each measure of x & y has uncertainties & correlation) with some additional tweaking

## prepare Julia environment {.unnumbered}

```{julia}
#| eval: false
import Pkg
Pkg.add(["Turing", "StatsPlots", "LogExpFunctions", "DataFrames", "KernelDensity", "MarkdownTables"])
```

```{julia}
using Statistics: mean, var, quantile
using LinearAlgebra: dot, diagm, det, eigen
using LogExpFunctions: logaddexp, logsumexp
using KernelDensity: kde
using DataFrames: DataFrame, eachrow
using Turing, StatsPlots, MarkdownTables
```

data entry
```{julia}
const N = 20;
const x    = [201., 244.,  47., 287., 203.,  58., 210., 202., 198., 158., 165., 201., 157., 131., 166., 160., 186., 125., 218., 146.];
const y    = [592., 401., 583., 402., 495., 173., 479., 504., 510., 416., 393., 442., 317., 311., 400., 337., 423., 334., 533., 344.];
const œÉ_x  = [  9.,   4.,  11.,   7.,   5.,   9.,   4.,   4.,  11.,   7.,   5.,   5.,   5.,   6.,   6.,   5.,   9.,   8.,   6.,   5.];
const œÉ_y  = [ 61.,  25.,  38.,  15.,  21.,  15.,  27.,  14.,  30.,  16.,  14.,  25.,  52.,  16.,  34.,  31.,  42.,  26.,  16.,  22.];
const œÅ_xy = [-.84,  .31,  .64, -.27, -.33,  .67, -.02, -.05, -.84, -.69,   .3, -.46, -.03,   .5,  .73, -.52,   .9,   .4, -.78, -.56];
```

data but in tensor format, to be useful in sections 7 & 8
```{julia}
const Z = [[x[i], y[i]] for i ‚àà 1:N];
const S = [ # covariance tensor
	[
		œÉ_x[i]^2                œÅ_xy[i]*œÉ_x[i]*œÉ_y[i];
		œÅ_xy[i]*œÉ_x[i]*œÉ_y[i]   œÉ_y[i]^2
	]
	for i ‚àà 1:N
];
```

data points 5 through 20 in the table, which exclude outliers, to be used in various exercises
```{julia}
const N¬¥ = 16;
const x¬¥    = x[   5:end];
const y¬¥    = y[   5:end];
const œÉ_x¬¥  = œÉ_x[ 5:end];
const œÉ_y¬¥  = œÉ_y[ 5:end];
const œÅ_xy¬¥ = œÅ_xy[5:end];
const Z¬¥    = Z[   5:end];
const S¬¥    = S[   5:end];
```

options to run MCMC chains
```{julia}
const N_samples = 5000;
const N_chains = 4;
const N_warmup = 1000;
const N_thinning = 5;
const N_bins = max(isqrt(N_samples), N_samples √∑ 10); # in histogram
```

helper functions
```{julia}
const angle90 = œÄ / 2; # useful in sections 7 & 8

function print_uncertainty(ùï©, Œ¥ùï©; digits=2)
	return "$(round(ùï©; digits=digits)) ¬± $(round(Œ¥ùï©; digits=digits))"
end

function plot_ellipses(‚Ñï, ùï©, ùï™, ‚Ñ§, ùïä)
	p = scatter(ùï©, ùï™, label=nothing)
	for i ‚àà 1:‚Ñï
		covellipse!(p, ‚Ñ§[i], ùïä[i], label=nothing)
	end
	return p
end

# formatter to use with MarkdownTables
value_to_markdown(ùï©::Number) = "$(round(ùï©; digits=3))"
value_to_markdown(ùï©::AbstractString) = ùï©
value_to_markdown(ùï©) = "`$ùï©`"

function print_summary(mcmc_chains)
	return markdown_table(summarystats(mcmc_chains)[:, [:mean, :std, :mcse, :rhat]]; formatter=value_to_markdown)
end

function print_information_criterion(data_model, mcmc_chains)
	# posterior log likehood
	# copied from https://discourse.julialang.org/t/likelihood-from-turing-model/102022/6
	post_log‚Ñí = [loglikelihood(data_model, row) for row ‚àà eachrow(DataFrame(mcmc_chains))]

	# Deviance Information Criterion
	# copied from https://github.com/StatisticalRethinkingJulia/ParetoSmoothedImportanceSampling.jl/blob/master/src/dic.jl
	deviance = (-2) .* post_log‚Ñí
	DIC = mean(deviance) + var(deviance) / 2

	# Widely Applicable Information Criterion
	# copied from https://github.com/StatisticalRethinkingJulia/ParetoSmoothedImportanceSampling.jl/blob/master/src/waic.jl
	lpd = dropdims(logsumexp(post_log‚Ñí .- log(size(post_log‚Ñí)[1]); dims=1); dims=1)
	pD = dropdims(var(post_log‚Ñí; dims=1, corrected=false); dims=1)
	WAIC = sum((-2) .* (lpd - pD))
	LPD = sum(lpd)
	penalty = sum(pD)

	println("Deviance Information Criterion: $DIC")
	println("Widely Applicable Information Criterion: $WAIC")
	println("Log predictive density: $LPD")
	println("overfitting penalty: $penalty")
end
```

quick look at data
```{julia}
scatter(x, y, xerror=œÉ_x, yerror=œÉ_y, label=nothing)
```

```{julia}
plot_ellipses(N, x, y, Z, S)
```

# Standard practice

## quick summary

conventional method for fitting a straight line $y=m\,x+b$ to data points where only the y-values have known Gaussian uncertainties

Construct the matrices:
$$
Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix}
\quad
A = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_N \end{bmatrix}
\quad
C = \begin{bmatrix}
\sigma_{y_1}^{2} & 0                & \cdots & 0 \\
0                & \sigma_{y_2}^{2} & \cdots & 0 \\
\vdots           & \vdots           & \ddots & \vdots \\
0                & 0                & \cdots & \sigma_{y_N}^{2}
\end{bmatrix}
$$

best-fit slope $m$ and intercept $b$ is given by: $\begin{bmatrix} b \\ m \end{bmatrix} = X = \left[A^\top\,C^{-1}\,A\right]^{-1}\,\left[A^\top\,C^{-1}\,Y\right]$

## exercise 01

::: {.callout-note}

Using the standard linear algebra method, fit the straight line $y=m\,x+b$ to the $x$, $y$, and $\sigma_y$ values for data points 5 through 20 in the table. That is, ignore the first 4 data points, and also ignore the columns for $\sigma_x$ and $\rho_{xy}$. Make a plot showing the points, their uncertainties, and the best-fit line. What is the standard uncertainty variance $\sigma_m^2$ on the slope of the line?

:::

```{julia}
Y‚Çë‚Çì‚ÇÄ‚ÇÅ = reshape(y¬¥, :, 1); # vector to 1-column matrix
A‚Çë‚Çì‚ÇÄ‚ÇÅ = hcat(ones(N¬¥), x¬¥);
C‚Çë‚Çì‚ÇÄ‚ÇÅ = diagm(œÉ_y¬¥ .^ 2);

tmp = A‚Çë‚Çì‚ÇÄ‚ÇÅ' * inv(C‚Çë‚Çì‚ÇÄ‚ÇÅ); # intermediary result
cov_bm = inv(tmp * A‚Çë‚Çì‚ÇÄ‚ÇÅ); # covariance matrix of b & m
X‚Çë‚Çì‚ÇÄ‚ÇÅ = cov_bm * tmp * Y‚Çë‚Çì‚ÇÄ‚ÇÅ;
b‚Çë‚Çì‚ÇÄ‚ÇÅ, m‚Çë‚Çì‚ÇÄ‚ÇÅ = X‚Çë‚Çì‚ÇÄ‚ÇÅ;
```

```{julia}
scatter(x¬¥, y¬¥, yerror=œÉ_y¬¥, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label=nothing)
```

The ‚Äústandard uncertainty variance‚Äù is to be found in the diagonals of the matrix: $\begin{bmatrix} \sigma_b^2 & \sigma_{mb} \\ \sigma_{mb} & \sigma_m^2 \end{bmatrix} = \left[A^\top\,C^{-1}\,A\right]^{-1}$
```{julia}
cov_bm
```

standard uncertainty (square root of variance) on b is `{julia} sqrt(cov_bm[1, 1])`, on m is `{julia} sqrt(cov_bm[2, 2])`

*i.e.* $b=$ `{julia} print_uncertainty(b‚Çë‚Çì‚ÇÄ‚ÇÅ, sqrt(cov_bm[1, 1]))` and $m=$ `{julia} print_uncertainty(m‚Çë‚Çì‚ÇÄ‚ÇÅ, sqrt(cov_bm[2, 2]))`

## exercise 02

::: {.callout-note}

Repeat exercise 01 but for all the data points in the table. What is the standard uncertainty variance $\sigma_m^2$ on the slope of the line? Is there anything you don‚Äôt like about the result? Is there anything different about the new points you have included beyond those used in exercise 01?

:::

```{julia}
Y‚Çë‚Çì‚ÇÄ‚ÇÇ = reshape(y, :, 1); # vector to 1-column matrix
A‚Çë‚Çì‚ÇÄ‚ÇÇ = hcat(ones(N), x);
C‚Çë‚Çì‚ÇÄ‚ÇÇ = diagm(œÉ_y .^ 2);

tmp = A‚Çë‚Çì‚ÇÄ‚ÇÇ' * inv(C‚Çë‚Çì‚ÇÄ‚ÇÇ); # intermediary result
cov_bm = inv(tmp * A‚Çë‚Çì‚ÇÄ‚ÇÇ); # covariance matrix of b & m
X‚Çë‚Çì‚ÇÄ‚ÇÇ = cov_bm * tmp * Y‚Çë‚Çì‚ÇÄ‚ÇÇ;
b‚Çë‚Çì‚ÇÄ‚ÇÇ, m‚Çë‚Çì‚ÇÄ‚ÇÇ = X‚Çë‚Çì‚ÇÄ‚ÇÇ;

scatter(x, y, yerror=œÉ_y, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label="exr 01")
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÇ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÇ, label="exr 02")
```

new values for the fitted line: $b=$ `{julia} print_uncertainty(b‚Çë‚Çì‚ÇÄ‚ÇÇ, sqrt(cov_bm[1, 1]))` and $m=$ `{julia} print_uncertainty(m‚Çë‚Çì‚ÇÄ‚ÇÇ, sqrt(cov_bm[2, 2]))`

the outliers drastically affect the fit:

- the slope is reduced by more than half and the intercept is adjusted by ~200 to compensate.
- uncertainties actually decrease

## exercise 03

::: {.callout-note}

Generalize the method of this section to fit a general quadratic (2nd order) relationship. Add another column to matrix $A$ containing the values $x_i^2$, and another element to vector $X$ (call it $q$). Then re-do exercise 01 butfitting for and plotting the best quadratic relationship $q\,x^2 + m\,x + b$

:::

```{julia}
Y‚Çë‚Çì‚ÇÄ‚ÇÉ = reshape(y¬¥, :, 1); # vector to 1-column matrix
A‚Çë‚Çì‚ÇÄ‚ÇÉ = hcat(ones(N¬¥), x¬¥, x¬¥.^2);
C‚Çë‚Çì‚ÇÄ‚ÇÉ = diagm(œÉ_y¬¥ .^ 2);

tmp = A‚Çë‚Çì‚ÇÄ‚ÇÉ' * inv(C‚Çë‚Çì‚ÇÄ‚ÇÉ); # intermediary result
cov_bmq = inv(tmp * A‚Çë‚Çì‚ÇÄ‚ÇÉ); # covariance matrix of b, m & q
X‚Çë‚Çì‚ÇÄ‚ÇÉ = cov_bmq * tmp * Y‚Çë‚Çì‚ÇÄ‚ÇÉ;
b‚Çë‚Çì‚ÇÄ‚ÇÉ, m‚Çë‚Çì‚ÇÄ‚ÇÉ, q‚Çë‚Çì‚ÇÄ‚ÇÉ = X‚Çë‚Çì‚ÇÄ‚ÇÉ;

scatter(x¬¥, y¬¥, yerror=œÉ_y¬¥, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label="exr 01")
plot!(ùï© -> q‚Çë‚Çì‚ÇÄ‚ÇÉ*ùï©^2 + m‚Çë‚Çì‚ÇÄ‚ÇÉ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÉ, label="exr 03")
```

$b=$ `{julia} print_uncertainty(b‚Çë‚Çì‚ÇÄ‚ÇÉ, sqrt(cov_bmq[1, 1]))` and $m=$ `{julia} print_uncertainty(m‚Çë‚Çì‚ÇÄ‚ÇÉ, sqrt(cov_bmq[2, 2]))` and $q=$ `{julia} print_uncertainty(q‚Çë‚Çì‚ÇÄ‚ÇÉ, sqrt(cov_bmq[3, 3]); digits=4)`

# The objective function

## quick summary

objective function: to measure how well a model fits the data ‚Üí optimize this function to find best-fit model

for the standard line-fitting case (Gaussian errors in y), best-fit model = maximizing the likelihood of observing the data given the line parameters $(m, b)$ which also minimizes the $\chi^2$ value, which represents the weighted sum of squared differences between the data and the fitted line

## exercise 04

::: {.callout-note}

Imagine a set of $N$ measurements $t_i$, with uncertainty variances $\sigma_{t_i}^2$, all of the same (unknown) quantity $T$. Assuming the generative model that each $t_i$ differs from $T$ by a Gaussian-distributed offset, taken from a Gaussian with zero mean and variance $\sigma_{t_i}^2$, write down an expression for the log likelihood $\log \mathcal{L}$ for the data given the model parameter $T$. Take a derivative and show that the maximum likelihood value for $T$ is the usual weighted mean.

:::

Gaussian likelihood:
$$
\mathcal{L} = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma_{t_i}^2}} \exp{\left(- \frac{(t_i - T)^2}{2\sigma_{t_i}^2}\right)}
$$

log-likelihood: when we are faced with the situation of taking the product of (potentially many) values close to zero, it‚Äôs a good idea to take the logarithm and sum the values instead:
$$
\log{\mathcal{L}} = -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log{\sigma_{t_i}^2} - \sum_{i=1}^N \frac{(t_i - T)^2}{2\sigma_{t_i}^2} = \mathrm{K} - \frac{1}{2} \chi^2
$$

derivative:
$$
\frac{\partial \log{\mathcal{L}}}{\partial T} = -\frac{\partial}{\partial T} \sum_{i=1}^N \frac{(t_i - T)^2}{{2\sigma_{t_i}^2}} = \sum_{i=1}^N \frac{(t_i - T)}{\sigma_{t_i}^2}
$$

This takes its maximum when the derivative is $0$
$$
\sum_{i=1}^N \frac{(t_i - T)}{\sigma_{t_i}^2} = 0 = \sum_{i=1}^N \frac{t_i}{\sigma_{t_i}^2} -T\sum_{i=1}^N \frac{1}{\sigma_{t_i}^2}
$$

we get the weighted mean as expected
$$
\hat{T} = \frac{\sum_{i=1}^N \frac{t_i}{\sigma_{t_i}^2}}{\sum_{i=1}^N \frac{1}{\sigma_{t_i}^2}}
$$

## exercise 05

::: {.callout-note}

Take the matrix formulation for $\chi^2$ given in equation (7) and take derivatives to show that the minimum is at the matrix location given in equation (5)

:::

*Equation (7)* is
$$
\chi^2 = [Y-AX]^\top C^{-1}[Y-AX]
$$
By the product rule, we have
$$
\frac{\partial}{\partial X} \chi^2 = \left(\frac{\partial}{\partial X}[Y-AX]^\top\right)C^{-1}[Y-AX] + [Y-AX]^\top\left(\frac{\partial}{\partial X} C^{-1}[Y-AX]\right)
$$
Differentiating, we get
$$
\frac{\partial}{\partial X} \chi^2 = -A^\top C^{-1}[Y-AX] [Y-AX]^\top C^{-1}[-A]
$$
Multiplying out
$$
\frac{\partial}{\partial X} \chi^2 = -A^\top C^{-1}Y + A^\top C^{-1}AX - Y^\top C^{-1}A + [AX]^\top C^{-1}A
$$
Since $-A^\top C^{-1}Y$ is a scalar, it equals its own transpose
$$
\frac{\partial}{\partial X} \chi^2 = -2A^\top C^{-1}Y + A^\top C^{-1}AX + X^\top A^\top C^{-1}A
$$
The two terms in $X$ are also simply transposes of one another, and can thus be trivially added together
$$
\frac{\partial}{\partial X} \chi^2 = -2A^\top C^{-1}Y + 2 A^\top C^{-1}AX
$$
$X$ takes its maximum likelihood value when the derivative is $0$:
$$
-2A^\top C^{-1}Y + 2 A^\top C^{-1}A\hat{X} = 0
$$
Rearranging:
$$
A^\top C^{-1}A\hat{X} = 2A^\top C^{-1}Y
$$
And so
$$
\hat{X} = \left(A^\top C^{-1}A\right)^{-1}A^\top C^{-1}Y
$$
As given in *equation (5)*

# Pruning outliers

## quick summary

importance of modeling outliers (data points that don‚Äôt fit the assumed model) rather than simply ignoring them

one of the well-known methods to deal with outliers: sigma clipping (iteratively removing points 1œÉ/3œÉ/5œÉ far from the fit then re-fit then remove until stable) ‚Üí a procedure which works very well but doesn‚Äôt an objective function top be optimized, so not statiscally justifiable

authors‚Äô proposition: mixture model

- outliers come from a distribution with probability $P_{\mathrm{bad}}$, example of distribution: $\mathcal{N}\left(Y_{\mathrm{bad}},V_{\mathrm{bad}}\right)$
- inliers come from straight line with probability $1-P_{\mathrm{bad}}$, therefore distribution: $\mathcal{N}\left(mx_i+b,\sigma_{y_i}^2\right)$

We now model our data as being drawn from a mixture of 2 Gaussians, one which is the ‚Äòtrue‚Äô relation and one which is a distribution of outliers. This mixture model is generated by marginalising an ‚Äòexponential‚Äô model which contains $N$ latent classifying labels $q_i$ for each data point
$$
\mathcal{L} \propto \prod_{i=1}^N\left[
\frac{1-P_{\mathrm{bad}}}{\sqrt{2\pi\sigma_{y_i}^2}} \exp\left(-\frac{[y_i-mx_i-b]^2}{2\sigma_{y_i}^2}\right) +
\frac{P_{\mathrm{bad}}}{\sqrt{2\pi[V_{\mathrm{bad}}+\sigma_{y_i}^2]}} \exp\left(-\frac{[y_i-Y_{\mathrm{bad}}]^2}{2[V_{\mathrm{bad}}+\sigma_{y_i}^2]}\right)
\right]
$$

## exercise 06

::: {.callout-note}

Using the mixture model proposed in the paper ‚Äî that treats the distribution as a mixture of a thin line containing a fraction $[1-P_{\mathrm{bad}}]$ of the points and a broader Gaussian containing a fraction $P_{\mathrm{bad}}$ of the points ‚Äî find the best-fit (the maximum a posteriori) straight line $y=m\,x+b$ for the $x$, $y$, and $\sigma_y$ for the data.

Before choosing the MAP line, marginalize over parameters $(P_{\mathrm{bad}},Y_{\mathrm{bad}},V_{\mathrm{bad}})$. That is, if you take a sampling approach, this means sampling the full 5-dimensional parameter space but then choosing the peak value in the histogram of samples in the 2-dimensional parameter space $(m,b)$.

Make one plot showing this 2-dimensional histogram, and another showing the points, their uncertainties, and the MAP line.

How does this compare to the standard result you obtained in exercise 02? Do you like the MAP line better or worse?

For extra credit, plot a sampling of 10 lines drawn from the marginalized posterior distribution for $(m,b)$ (marginalized over $(P_{\mathrm{bad}},Y_{\mathrm{bad}},V_{\mathrm{bad}})$) and plot the samples as a set of light grey or ransparent lines.

:::

```{julia}
@model function model‚Çë‚Çì‚ÇÄ‚ÇÜ(‚Ñï, ùï©, ùï™, œÉ_ùï™)
	b ~ Normal(0, 5)
	m ~ Normal(0, 5)

	# for outliers
	P_bad ~ Uniform(0, 1)
	Y_bad ~ Normal(0, 10) # mean for all outliers
	V_bad ~ InverseGamma(.001, .001) # additional variance for outliers

	# faster computation: marginalize discrete param
	for i ‚àà 1:‚Ñï
		ùï™ÃÇ·µ¢ = b + m * ùï©[i]
		œÉ_bad·µ¢ = sqrt(œÉ_ùï™[i]^2 + V_bad)

		## method 1: manually modify log likehood
		#=
		 inlier = log1p(-P_bad) + logpdf(Normal(ùï™ÃÇ·µ¢   , œÉ_ùï™[i]), ùï™[i])
		outlier = log(   P_bad) + logpdf(Normal(Y_bad, œÉ_bad·µ¢), ùï™[i])
		Turing.@addlogprob! logaddexp(inlier, outlier)
		=#

		## medthod 2: use convenient constructors for mixture models
		ùï™[i] ~ MixtureModel(Normal, [(ùï™ÃÇ·µ¢, œÉ_ùï™[i]), (Y_bad, œÉ_bad·µ¢)], [1 - P_bad, P_bad])

	end
end

data_model‚Çë‚Çì‚ÇÄ‚ÇÜ = model‚Çë‚Çì‚ÇÄ‚ÇÜ(N, x, y, œÉ_y)
chains‚Çë‚Çì‚ÇÄ‚ÇÜ = sample(data_model‚Çë‚Çì‚ÇÄ‚ÇÜ, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
```

diagnostics of chains:
```{julia}
print_information_criterion(data_model‚Çë‚Çì‚ÇÄ‚ÇÜ, chains‚Çë‚Çì‚ÇÄ‚ÇÜ)
print_summary(chains‚Çë‚Çì‚ÇÄ‚ÇÜ)
```
all chains seem converged, we‚Äôre good!

good thing about MCMC is that it samples directly from marginalized distribution so no need to compute any integral after running
```{julia}
p = plot(layout=2)
histogram2d!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b], chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m], bins=N_bins, normalize=:pdf, color=:plasma, subplot=1, xlabel="b", ylabel="m")
plot!(p, kde((reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b].data), reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m].data))), subplot=2, xlabel="b", ylabel="m")
```

```{julia}
#| echo: false
#| eval: false

# plot(chains‚Çë‚Çì‚ÇÄ‚ÇÜ, seriestype=:mixeddensity) # cannot change layout

p = plot(layout=(3, 2), legend=false)

density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b], subplot=1, title="b")
density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m], subplot=3, title="m")
histogram2d!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b], chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m], subplot=5, bins=N_bins, normalize=:pdf, color=:plasma, xlabel="b", ylabel="m")

density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:Y_bad], subplot=2, label=nothing, title="Y_bad")
density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:P_bad], subplot=4, label=nothing, title="P_bad")
density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:V_bad], subplot=6, label=nothing, title="V_bad")
```

```{julia}
b‚Çë‚Çì‚ÇÄ‚ÇÜ = mean(chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b]);
m‚Çë‚Çì‚ÇÄ‚ÇÜ = mean(chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m]);
scatter(x, y, yerror=œÉ_y, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÇ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÇ, label="exr 02")
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÜ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÜ, label="exr 06")
```

## exercise 07

::: {.callout-note}

Solve exercise 06 but now plot the fully marginalized (over $m,b,Y_{\mathrm{bad}},V_{\mathrm{bad}}$) posterior distribution function for parameter $P_{\mathrm{bad}}$. Is this distribution peaked about where you would expect, given the data?

Now repeat the problem, but dividing all the data uncertainty variances $\sigma_{yi}^2$ by 4 (or dividing the uncertainties $\sigma_{yi}$ by 2). Again plot the fully marginalized posterior distribution function for parameter $P_{\mathrm{bad}}$. Discuss.

:::

```{julia}
data_model‚Çë‚Çì‚ÇÄ‚Çá = model‚Çë‚Çì‚ÇÄ‚ÇÜ(N, x, y, œÉ_y ./ 2)
chains‚Çë‚Çì‚ÇÄ‚Çá = sample(data_model‚Çë‚Çì‚ÇÄ‚Çá, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÄ‚Çá, chains‚Çë‚Çì‚ÇÄ‚Çá)
print_summary(chains‚Çë‚Çì‚ÇÄ‚Çá)
```
```{julia}
p = plot(layout=2)
density!(p, chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:P_bad], subplot=1, label=nothing, title="P_bad with original error")
density!(p, chains‚Çë‚Çì‚ÇÄ‚Çá[:P_bad], subplot=2, label=nothing, title="P_bad with half error")
```
with original error: peak at 0.2, arguably 3-4 outliers in the data, so it makes sense

with ¬Ω original error: bigger value for the peak meaning that many more points have been considered outliers

## little extra

compute probability of each point to be classified as outlier

```{julia}
# Compute the un-normalized log probabilities for each cluster
log_prob_assign_outliers = zeros(N_chains, N_samples, N);
Threads.@threads for step ‚àà 1:N_samples
	for chain ‚àà 1:N_chains
		b = chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:b][step, chain]
		m = chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:m][step, chain]
		V_bad = chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:V_bad][step, chain]
		P_bad = chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:P_bad][step, chain]
		Y_bad = chains‚Çë‚Çì‚ÇÄ‚ÇÜ[:Y_bad][step, chain]
		for i ‚àà 1:N
			≈∑·µ¢ = b + m * x[i]
			inliers_log_prob = log1p(-P_bad) + logpdf(Normal(≈∑·µ¢, œÉ_y[i]), y[i])

			œÉ_bad·µ¢ = sqrt(œÉ_y[i]^2 + V_bad)
			outliers_log_prob = log(P_bad) + logpdf(Normal(Y_bad, œÉ_bad·µ¢), y[i])

			# Bayes rule to compute the assignment probability: P(cluster = 1 | data) ‚àù P(data | cluster = 1) P(cluster = 1)
			log_prob_assign_outliers[chain, step, i] = outliers_log_prob - logaddexp(inliers_log_prob, outliers_log_prob)
		end
	end
end

log_prob_assign_outliers_bis = dropdims(logsumexp(log_prob_assign_outliers; dims=2); dims=2); # shape: N_chains √ó N
proba_outlier = exp.(log_prob_assign_outliers_bis .- log(N_samples));

# Average across the MCMC chain
for proba ‚àà mean(proba_outlier; dims=1)
	println("$(round(100*proba; digits=2)) %")
end
```

# Uncertainties in the best-fit parameters

## quick summary

This brief section reiterates how to determine the uncertainties (often represented by a covariance matrix) associated with the best-fit parameters obtained from the fitting process.

It stresses that the validity of these uncertainty estimates depends heavily on the correctness of the initial assumptions about the data‚Äôs error properties and the appropriateness of the model itself.

## exercise 08

::: {.callout-note}

Compute the standard uncertainty $\sigma_m^2$ obtained for the slope of the line found by the standard fit you did in exercise 02. Now make jackknife (20 trials) and bootstrap estimates for the uncertainty $\sigma_m^2$. How do the uncertainties compare and which seems most reasonable, given the data and uncertainties on the data?

:::

lorem ipsum

## exercise 09

::: {.callout-note}

Re-do exercise 06 ‚Äî the mixture-based outlier model ‚Äî but just with the ‚Äúinlier‚Äù points 5 through 20 from the data. Then do the same again, but with all measurement uncertainties reduced by a factor of 2 (uncertainty variances reduced by a factor of 4). Plot the marginalized posterior probability distributions for line parameters $(m,b)$ in both cases.

Did these posterior distributions get smaller or larger with the reduction in the data-point uncertainties? Compare this with the dependence of the standard uncertainty estimate $\left[A^\top\,C^{-1}\,A\right]^{-1}$.

:::

```{julia}
data_model‚Çë‚Çì‚ÇÄ‚Çâ = model‚Çë‚Çì‚ÇÄ‚ÇÜ(N¬¥, x¬¥, y¬¥, œÉ_y¬¥)
chains‚Çë‚Çì‚ÇÄ‚Çâ = sample(data_model‚Çë‚Çì‚ÇÄ‚Çâ, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÄ‚Çâ, chains‚Çë‚Çì‚ÇÄ‚Çâ)
print_summary(chains‚Çë‚Çì‚ÇÄ‚Çâ)
```

```{julia}
data_model‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr = model‚Çë‚Çì‚ÇÄ‚ÇÜ(N¬¥, x¬¥, y¬¥, œÉ_y¬¥ ./ 2)
chains‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr = sample(data_model‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr, chains‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr)
print_summary(chains‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr)
```

```{julia}
p = plot(layout=2)
plot!(p, kde((reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚Çâ[     :b].data), reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚Çâ[     :m].data))), subplot=1, xlabel="b", ylabel="m", title="with original error")
plot!(p, kde((reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr[:b].data), reduce(vcat, chains‚Çë‚Çì‚ÇÄ‚Çâ_¬Ωerr[:m].data))), subplot=2, xlabel="b", ylabel="m", title="with half error")
```

The posterior distributions got much larger with the reduction in uncertainties. They‚Äôre way underestimated.

If individual data-points have been estimated by some means that effectively relies on shared information, then there will be large covariances among the data points. These covariances bring off-diagonal elements into the covariance matrix $C$, which was previously trivially constructed under the assumption that all covariances are precisely $0$.

Once the off-diagonal elements are non-zero, $\chi^2$ must be computed by the matrix expression: $\chi^2 = [Y-AX]^\top C^{-1}[Y-AX]$

# Non-Gaussian uncertainties

*no exercise in this section*

quick summary: can somehow model as gaussian anyway ‚Ä¶

# Goodness of fit and unknown uncertainties

## quick summary

standard (frequentist) paradigm: $\chi^2$ to measure goodness of fit

with a straight line: $\chi^2$ expect to be in between $[N-2] \pm \sqrt{2[N-2]}$ with 2 is model parameters count ($m$ & $b$)

## exercise 10

::: {.callout-note}

Assess the $\chi^2$ value for the fit performed in exercise 01 (do that problem first if you haven‚Äôt already). Is the fit good?  What about for the fit performed in exercise 02?

:::

```{julia}
tmp = Y‚Çë‚Çì‚ÇÄ‚ÇÅ - A‚Çë‚Çì‚ÇÄ‚ÇÅ * X‚Çë‚Çì‚ÇÄ‚ÇÅ; # intermediary result
œá¬≤‚Çë‚Çì‚ÇÄ‚ÇÅ = tmp' * inv(C‚Çë‚Çì‚ÇÄ‚ÇÅ) * tmp;
œá¬≤‚Çë‚Çì‚ÇÄ‚ÇÅ / (N¬¥ - 1) # œá¬≤ per degrees of freedom
```
This is a pretty good fit. Aside from the plot showing a linear fit that generally follows the points (whether the points themselves are linear or not), the reduced $\chi^2$ metric is pretty close to 1. This is indicating that on average the squared difference between my model and each point is roughly on par (actually slightly larger) than the measured inherent variance of each point. Can't ask for too much better than that when you're assuming the functional form of a distribution.

```{julia}
tmp = Y‚Çë‚Çì‚ÇÄ‚ÇÇ - A‚Çë‚Çì‚ÇÄ‚ÇÇ * X‚Çë‚Çì‚ÇÄ‚ÇÇ; # intermediary result
œá¬≤‚Çë‚Çì‚ÇÄ‚ÇÇ = tmp' * inv(C‚Çë‚Çì‚ÇÄ‚ÇÇ) * tmp;
œá¬≤‚Çë‚Çì‚ÇÄ‚ÇÇ / (N - 1) # œá¬≤ per degrees of freedom
```
This is a horrid fit. Squared differences 15√ó larger than what‚Äôs described by measured variances.

## exercise 11

::: {.callout-note}

Re-do the fit of exercise 01 but setting all $\sigma_{y_i}^2=S$, that is, ignoring the uncertainties and replacing them all with the same value $S$. What uncertainty variance $S$ would make $\chi^2 = N-2$? How does it compare to the mean and median of the uncertainty variances $\left(\sigma_{y_i}^2\right)_{i=1}^N$?

:::

```{julia}
Y‚Çë‚Çì‚ÇÅ‚ÇÅ = reshape(y¬¥, :, 1); # vector to 1-column matrix
A‚Çë‚Çì‚ÇÅ‚ÇÅ = hcat(ones(N¬¥), x¬¥);

S‚Çë‚Çì‚ÇÅ‚ÇÅ = 1:2000;
œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ = map(S‚Çë‚Çì‚ÇÅ‚ÇÅ) do s
	œÉ¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ = fill(s, N¬¥)
	C‚Çë‚Çì‚ÇÅ‚ÇÅ = diagm(œÉ¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ)
	tmp1 = inv(C‚Çë‚Çì‚ÇÅ‚ÇÅ)
	tmp2 = A‚Çë‚Çì‚ÇÅ‚ÇÅ' * tmp1
	X‚Çë‚Çì‚ÇÅ‚ÇÅ = inv(tmp2 * A‚Çë‚Çì‚ÇÅ‚ÇÅ) * tmp2 * Y‚Çë‚Çì‚ÇÅ‚ÇÅ
	tmp3 = Y‚Çë‚Çì‚ÇÅ‚ÇÅ - A‚Çë‚Çì‚ÇÅ‚ÇÅ * X‚Çë‚Çì‚ÇÅ‚ÇÅ
	return only(tmp3' * tmp1 * tmp3) # get scalar from 1√ó1 matrix
end

subsetNm2 = N¬¥ - 2
scatter(S‚Çë‚Çì‚ÇÅ‚ÇÅ, œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ, yaxis=:log, xlabel="S", ylabel="œá¬≤", label=nothing)
hline!([subsetNm2], label="œá¬≤=N-2")
vline!([mean(œÉ_y¬¥ .^ 2)], label="mean œÉ¬≤")
```

value S would be between:
```{julia}
S‚Çë‚Çì‚ÇÅ‚ÇÅ[findlast(ùï© -> ùï© > subsetNm2, œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ)]
```
and
```{julia}
S‚Çë‚Çì‚ÇÅ‚ÇÅ[findfirst(ùï© -> ùï© < subsetNm2, œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ)]
```

```{julia}
#| echo: false

S‚Çë‚Çì‚ÇÅ‚ÇÅbis = 938:.01:939;
œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅbis = map(S‚Çë‚Çì‚ÇÅ‚ÇÅbis) do s
	œÉ¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ = fill(s, N¬¥)
	C‚Çë‚Çì‚ÇÅ‚ÇÅ = diagm(œÉ¬≤‚Çë‚Çì‚ÇÅ‚ÇÅ)
	tmp1 = inv(C‚Çë‚Çì‚ÇÅ‚ÇÅ)
	tmp2 = A‚Çë‚Çì‚ÇÅ‚ÇÅ' * tmp1
	X‚Çë‚Çì‚ÇÅ‚ÇÅ = inv(tmp2 * A‚Çë‚Çì‚ÇÅ‚ÇÅ) * tmp2 * Y‚Çë‚Çì‚ÇÅ‚ÇÅ
	tmp3 = Y‚Çë‚Çì‚ÇÅ‚ÇÅ - A‚Çë‚Çì‚ÇÅ‚ÇÅ * X‚Çë‚Çì‚ÇÅ‚ÇÅ
	return only(tmp3' * tmp1 * tmp3) # get scalar from 1√ó1 matrix
end

scatter(S‚Çë‚Çì‚ÇÅ‚ÇÅbis, œá¬≤‚Çë‚Çì‚ÇÅ‚ÇÅbis, xlabel="S", ylabel="œá¬≤", label=nothing)
hline!([subsetNm2], label="œá¬≤=N-2")
```

mean variances $\left(\sigma_{y_i}^2\right)_{i=1}^N$ of the true data set: `{julia} mean(œÉ_y¬¥ .^ 2)`

The uniform offset $S$ necessary to make $\chi^2 = N-2$ is quite larger than both the mean and median variances $\left(\sigma_{y_i}^2\right)_{i=1}^N$ of the true data set. If the data were in fact linear with some measurement uncertainties, perhaps the uncertainties themselves are underestimated.

## exercise 12

::: {.callout-note}

Flesh out and write all equations for the Bayesian uncertainty estimation and marginalization procedure described in this section. Note that the inference and marginalization would be very expensive without excellent sampling tools! Make the additional (unjustified) assumption that all the uncertainties have the same variance $\sigma_{y_i}^2=S$ to make the problem tractable.

Apply the method to the $x$ and $y$ values for points 5 through 20 in the data. Make a plot showing the points, the maximum a posteriori value of the uncertainty variance as error bars, and the maximum a posteriori straight line.

For extra credit, plot 2 straight lines, one that is maximum a posteriori for the full posterior and one that is the same but for the posterior after the uncertainty variance $S$ has been marginalized out. Also plot 2 sets of error bars, one that shows the maximum for the full posterior and one for the posterior after the line parameters $(m,b)$ have been marginalized out.

:::

Gaussian likelihood:
$$
\mathcal{L} = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\mathrm{S}}} \exp{\left(- \frac{(y_i - mx_i - b)^2}{2\mathrm{S}}\right)}
$$

```{julia}
@model function model‚Çë‚Çì‚ÇÅ‚ÇÇ(‚Ñï, ùï©, ùï™)
	b ~ Normal(0, 5)
	m ~ Normal(0, 5)
	S ~ InverseGamma(.001, .001) # common variance for all data

	for i ‚àà 1:‚Ñï
		ùï™[i] ~ Normal(b + m*ùï©[i], sqrt(S))
	end
end

data_model‚Çë‚Çì‚ÇÅ‚ÇÇ = model‚Çë‚Çì‚ÇÅ‚ÇÇ(N¬¥, x¬¥, y¬¥)
chains‚Çë‚Çì‚ÇÅ‚ÇÇ = sample(data_model‚Çë‚Çì‚ÇÅ‚ÇÇ, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÅ‚ÇÇ, chains‚Çë‚Çì‚ÇÅ‚ÇÇ)
print_summary(chains‚Çë‚Çì‚ÇÅ‚ÇÇ)
```

```{julia}
b‚Çë‚Çì‚ÇÅ‚ÇÇ = mean(chains‚Çë‚Çì‚ÇÅ‚ÇÇ[:b]);
m‚Çë‚Çì‚ÇÅ‚ÇÇ = mean(chains‚Çë‚Çì‚ÇÅ‚ÇÇ[:m]);
S‚Çë‚Çì‚ÇÅ‚ÇÇ = mean(chains‚Çë‚Çì‚ÇÅ‚ÇÇ[:S]);
scatter(x¬¥, y¬¥, yerror=fill(sqrt(S‚Çë‚Çì‚ÇÅ‚ÇÇ), N¬¥), label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚ÇÇ*ùï© + b‚Çë‚Çì‚ÇÅ‚ÇÇ, label="exr 12")
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label="exr 01")
```

# Arbitrary 2-dimensional uncertainties

## quick summary

more complex scenario of fitting a line when both the x and y measurements have uncertainties: using a covariance tensor for each data point to represent the uncertainties in x, the uncertainties in y, and any correlation between the x and y errors

covariance tensor $S_i = \begin{bmatrix} \sigma_{x_i}^2 & \sigma_{xy_i} \\ \sigma_{xy_i} & \sigma_{y_i}^2 \end{bmatrix}$ with $\sigma_{xy}=\rho_{xy}\sigma_x\sigma_y$

likelihood
$$
\mathcal{L} = \prod_{i=1}^N \frac{1}{2\pi\sqrt{\det(S_i)}}\exp\left(-\frac{1}{2}\,\left[Z_i - Z\right]^\top S_i^{-1} \left[Z_i - Z\right]\right)
$$
with
$$
Z_i = \begin{bmatrix} x_i \\ y_i \end{bmatrix} \quad ; \quad
Z = \begin{bmatrix} \hat{x_i} \\ \hat{y_i} \end{bmatrix} = \begin{bmatrix} \hat{x_i} \\ m\hat{x_i}+b \end{bmatrix}
$$

instead of slope $m$, use unit vector $\vec{v}$ orthogonal to the line
$$
\vec{v} = \frac{1}{\sqrt{1+m^2}} \begin{bmatrix} -m \\ 1 \end{bmatrix} = \begin{bmatrix} -\sin\theta \\ \cos\theta \end{bmatrix}
$$
with $\theta = \arctan m$ angle made between the line and the $x$ axis

orthogonal displacement $\Delta_i$ of each data point $(x_i,y_i)$ from the line: $\Delta_i = \vec{v}^\top Z_i - b\cos\theta$

each data point‚Äôs covariance tensor $S_i$ projects down to an orthogonal variance $\Sigma_i^2 = \vec{v}^\top S_i \vec{v}$

then the log likelihood can be written as
$$
\log\mathcal{L} = K - \frac{1}{2}\sum_{i=1}^N \left[\frac{\Delta_i^2}{\Sigma_{i}^2} +\log{|\Sigma_{i}^2|} + \log{(1+m^2)} \right]
$$
where $K$ is some constant

the authors suggest: performing the fit or likelihood maximization not in terms of $(m,b)$ but rather $(\theta,b_\perp)$, where $b_\perp = b\cos\theta$ the perpendicular distance of the line from the origin

> In the astrophysics literature, there is a tradition, when there are uncertainties in both directions, of fitting the ‚Äúforward‚Äù and ‚Äúreverse‚Äù relations ‚Äî that is, fitting y as a function of x and then x as a function of y ‚Äî and then splitting the difference between the 2 slopes so obtained, or treating the difference between the slopes as a systematic uncertainty. This is unjustified.

> Another common method for finding the linear relationship in data when there are uncertainties in both directions is principal components analysis. The method of PCA does return a linear relationship for a data set, in the form of the dominant principal component. However, this is the dominant principal component of the observed data, not of the underlying linear relationship that, when noise is added, generates the observations. For this reason, the output of PCA will be strongly drawn or affected by the individual data point noise covariances $S_i$.

**example of mixture model with outlier**
$$
Z_{\mathrm{bad}} = \begin{bmatrix} X_{\mathrm{bad}} \\ Y_{\mathrm{bad}} \end{bmatrix} \quad ; \quad
V_{\mathrm{bad}} = \begin{bmatrix}
V_{\mathrm{bad}_x} & \rho_{\mathrm{bad}}\sqrt{V_{\mathrm{bad}_x}V_{\mathrm{bad}_y}} \\
\rho_{\mathrm{bad}}\sqrt{V_{\mathrm{bad}_x}V_{\mathrm{bad}_y}} & V_{\mathrm{bad}_y}
\end{bmatrix}
$$
$$
\mathcal{L} \propto \prod_{i=1}^N\left[
\frac{1-P_{\mathrm{bad}}}{\sqrt{2\pi\Sigma_{i}^2}} \cos(\theta) \exp\left(-\frac{\Delta_i^2}{2\Sigma_{i}^2}\right) +
\frac{P_{\mathrm{bad}}}{\sqrt{2\pi\det{(V_{\mathrm{bad}}+S_i)}}} \exp\left(-\frac{1}{2}\,\left[Z_i - Z_{\mathrm{bad}}\right]^\top \left[V_{\mathrm{bad}}+S_i\right]^{-1} \left[Z_i - Z_{\mathrm{bad}}\right]\right)
\right]
$$

## exercise 13

::: {.callout-note}

Using the method of this section, fit the straight line $y=m\,x+b$ to the $x$, $y$, $\sigma_x^2$, $\sigma_{xy}$, and $\sigma_y^2$ values of points 5 through 20 taken from the data. Make a plot showing the points, their 2-dimensional uncertainties (show them as 1œÉ ellipses), and the best-fit line.

:::

```{julia}
@model function model‚Çë‚Çì‚ÇÅ‚ÇÉ(‚Ñï, ‚Ñ§, ùïä)
	b ~ Normal(0, 5) # intercept
	Œ∏ ~ Uniform(-angle90, angle90) # angle of the fitted line, use this instead of slope
	v‚Éó = [-sin(Œ∏) cos(Œ∏)]' # unit vector orthogonal to the line

	for i ‚àà 1:‚Ñï
		Œî·µ¢ = dot(‚Ñ§[i], v‚Éó) - b*v‚Éó[1] # orthogonal displacement of each data point from the line
		Œ£·µ¢¬≤ = dot(v‚Éó', ùïä[i], v‚Éó) # orthogonal variance of projection of each data point to the line
		Turing.@addlogprob! -.5*(Œî·µ¢^2 / Œ£·µ¢¬≤ + log(abs(Œ£·µ¢¬≤)) + log1p(tan(Œ∏)^2))
	end
end

data_model‚Çë‚Çì‚ÇÅ‚ÇÉ = model‚Çë‚Çì‚ÇÅ‚ÇÉ(N¬¥, Z¬¥, S¬¥)
chains‚Çë‚Çì‚ÇÅ‚ÇÉ = sample(data_model‚Çë‚Çì‚ÇÅ‚ÇÉ, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÅ‚ÇÉ, chains‚Çë‚Çì‚ÇÅ‚ÇÉ)
print_summary(chains‚Çë‚Çì‚ÇÅ‚ÇÉ)
```

```{julia}
b‚Çë‚Çì‚ÇÅ‚ÇÉ = mean(chains‚Çë‚Çì‚ÇÅ‚ÇÉ[:b]);
m‚Çë‚Çì‚ÇÅ‚ÇÉ = tan(mean(chains‚Çë‚Çì‚ÇÅ‚ÇÉ[:Œ∏]));

plot_ellipses(N¬¥, x¬¥, y¬¥, Z¬¥, S¬¥)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚ÇÉ*ùï© + b‚Çë‚Çì‚ÇÅ‚ÇÉ, label="exr 13")
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label="exr 01")
```

## exercise 14

::: {.callout-note}

Repeat exercise 13, but using all of the data. Some of the points are now outliers, so your fit may look worse. Follow the fit by a robust procedure analogous to the Bayesian mixture model with bad-data
probability $P_{\mathrm{bad}}$ described in section Pruning outliers. Use something sensible for the prior probability distribution for $(m,b)$.

Plot the 2 results with the data and uncertainties.

For extra credit, plot a sampling of 10 lines drawn from the marginalized posterior distribution for $(m,b)$ and plot the samples as a set of light grey or transparent lines. For extra extra credit, mark each data point on your plot with the fully marginalized probability that the point is bad (that is, rejected, or has $q=0$).

:::

```{julia}
@model function model‚Çë‚Çì‚ÇÅ‚ÇÑ(‚Ñï, ‚Ñ§, ùïä)
	b ~ Normal(0, 5) # intercept
	Œ∏ ~ Uniform(-angle90, angle90) # angle of the fitted line, use this instead of slope
	v‚Éó = [-sin(Œ∏) cos(Œ∏)]' # unit vector orthogonal to the line

	# for outliers
	P_bad ~ Uniform(0, 1)
	X_bad ~ Normal(0, 10)
	Y_bad ~ Normal(0, 10)
	V_bad_x ~ InverseGamma(.001, .001)
	V_bad_y ~ InverseGamma(.001, .001)
	œÅ_bad ~ Uniform(-1, 1)

	Z_bad = [X_bad, Y_bad]
	V_bad = [
		V_bad_x                         œÅ_bad * sqrt(V_bad_x*V_bad_y);
		œÅ_bad * sqrt(V_bad_x*V_bad_y)   V_bad_y
	]

	for i ‚àà 1:‚Ñï
		Œî·µ¢ = dot(‚Ñ§[i], v‚Éó) - b*v‚Éó[1] # orthogonal displacement of each data point from the line
		Œ£·µ¢¬≤ = dot(v‚Éó', ùïä[i], v‚Éó) # orthogonal variance of projection of each data point to the line
		tmp1 = ‚Ñ§[i] - Z_bad
		tmp2 = ùïä[i] + V_bad
		inlier = log1p(-P_bad) - .5*log(abs(Œ£·µ¢¬≤)) + log(abs(v‚Éó[1])) - .5 * Œî·µ¢^2 / Œ£·µ¢¬≤
		outlier = log(P_bad) - .5*log(abs(det(tmp2))) - .5 * tmp1' * tmp2 * tmp1
		Turing.@addlogprob! logaddexp(inlier, outlier)
	end
end

data_model‚Çë‚Çì‚ÇÅ‚ÇÑ = model‚Çë‚Çì‚ÇÅ‚ÇÑ(N, Z, S)
chains‚Çë‚Çì‚ÇÅ‚ÇÑ = sample(data_model‚Çë‚Çì‚ÇÅ‚ÇÑ, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÅ‚ÇÑ, chains‚Çë‚Çì‚ÇÅ‚ÇÑ)
print_summary(chains‚Çë‚Çì‚ÇÅ‚ÇÑ)
```

```{julia}
b‚Çë‚Çì‚ÇÅ‚ÇÑ = mean(chains‚Çë‚Çì‚ÇÅ‚ÇÑ[:b]);
m‚Çë‚Çì‚ÇÅ‚ÇÑ = tan(mean(chains‚Çë‚Çì‚ÇÅ‚ÇÑ[:Œ∏]));

plot_ellipses(N, x, y, Z, S)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚ÇÑ*ùï© + b‚Çë‚Çì‚ÇÅ‚ÇÑ, label="exr 13")
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÇ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÇ, label="exr 02")
```

## exercise 15

::: {.callout-note}

Perform the abominable forward-reverse fitting procedure on points 5 through 20 from the data.

That is, fit a straight line to the $y$ values of the points, using the $y$-direction uncertainties $\sigma_y^2$ only, by the standard method described in section Standard practice.

Now transpose the problem and fit the same data but fitting the $x$ values using the $x$-direction uncertainties $\sigma_x^2$ only.

Make a plot showing the data points, the $x$-direction and $y$-direction uncertainties, and the 2 best-fit lines. Comment.

:::

lorem ipsum

## exercise 16

::: {.callout-note}

Perform principal components analysis on points 5 through 20 from the data. That is, diagonalize the 2√ó2 matrix $Q$ given by $Q = \sum_{i=1}^N\,\left[Z_i-\bar{Z}\right]\,{\left[Z_i-\bar{Z}\right]}^\top$ with $\bar{Z} = \frac{1}{N}\,\sum_{i=1}^N\,Z_i$. Find the eigenvector of $Q$ with the largest eigenvalue.

Now make a plot showing the data points, and the line that goes through the mean $\bar{Z}$ of the data with the slope corresponding to the direction of the principal eigenvector. Comment.

:::

```{julia}
ZÃÑ¬¥ = [mean(x¬¥), mean(y¬¥)];
Z¬¥_centered = [Z¬¥[i] - ZÃÑ¬¥ for i ‚àà 1:N¬¥];
Q = sum([z*z' for z ‚àà Z¬¥_centered]);

_, eigvecs = eigen(Q);
eigvecs_max = eigvecs[:, end]; # direction vector of 1st principal component
m‚Çë‚Çì‚ÇÅ‚ÇÜ = eigvecs_max[end] / eigvecs_max[begin];
b‚Çë‚Çì‚ÇÅ‚ÇÜ = ZÃÑ¬¥[2] - m‚Çë‚Çì‚ÇÅ‚ÇÜ * ZÃÑ¬¥[1]; # 1st principal component goes through the means

scatter(x¬¥, y¬¥, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚ÇÜ*ùï© + b‚Çë‚Çì‚ÇÅ‚ÇÜ, label=nothing)
```
slope = `{julia} round(m‚Çë‚Çì‚ÇÅ‚ÇÜ; digits=3)` and intercept = `{julia} round(b‚Çë‚Çì‚ÇÅ‚ÇÜ; digits=3)`

# Intrinsic scatter

## quick summary

introduce an intrinsic Gaussian variance $V$, orthogonal to the line

the log likelihood can be written as
$$
\log\mathcal{L} = K - \frac{1}{2}\sum_{i=1}^N \left[\frac{\Delta_i^2}{\Sigma_{i}^2+V} +\log{|\Sigma_{i}^2+V|} + \log{(1+m^2)} \right]
$$
where $K$ is some constant

limitations: it models only the distribution orthogonal to the relationship

## exercise 17

::: {.callout-note}

Re-do exercise 13, but now allowing for an orthogonal intrinsic Gaussian variance $V$ and only excluding data point 3. Re-make the plot, showing not the best-fit line but rather the $\pm\sqrt{V}$ lines for the maximum-likelihood intrinsic relation.

:::


```{julia}
@model function model‚Çë‚Çì‚ÇÅ‚Çá(‚Ñï, ‚Ñ§, ùïä)
	b ~ Normal(0, 5) # intercept
	Œ∏ ~ Uniform(-angle90, angle90) # angle of the fitted line, use this instead of slope
	v‚Éó = [-sin(Œ∏) cos(Œ∏)]' # unit vector orthogonal to the line
	V ~ InverseGamma(.001, .001) # intrinsic Gaussian variance orthogonal to the line

	for i ‚àà 1:‚Ñï
		Œî·µ¢ = dot(‚Ñ§[i], v‚Éó) - b*v‚Éó[1] # orthogonal displacement of each data point from the line
		Œ£·µ¢¬≤ = dot(v‚Éó', ùïä[i], v‚Éó) # orthogonal variance of projection of each data point to the line
		tmp = Œ£·µ¢¬≤ + V # intermediary result
		Turing.@addlogprob! -.5*(Œî·µ¢^2 / tmp + log(abs(tmp)) + log1p(tan(Œ∏)^2))
	end
end

data_model‚Çë‚Çì‚ÇÅ‚Çá = model‚Çë‚Çì‚ÇÅ‚Çá(N¬¥, Z¬¥, S¬¥)
chains‚Çë‚Çì‚ÇÅ‚Çá = sample(data_model‚Çë‚Çì‚ÇÅ‚Çá, NUTS(), MCMCThreads(), N_samples, N_chains; num_warmup=N_warmup, thinning=N_thinning);
print_information_criterion(data_model‚Çë‚Çì‚ÇÅ‚Çá, chains‚Çë‚Çì‚ÇÅ‚Çá)
print_summary(chains‚Çë‚Çì‚ÇÅ‚Çá)
```

```{julia}
b‚Çë‚Çì‚ÇÅ‚Çá = mean(chains‚Çë‚Çì‚ÇÅ‚Çá[:b]);
Œ∏‚Çë‚Çì‚ÇÅ‚Çá = mean(chains‚Çë‚Çì‚ÇÅ‚Çá[:Œ∏])
m‚Çë‚Çì‚ÇÅ‚Çá = tan(Œ∏‚Çë‚Çì‚ÇÅ‚Çá);
V‚Çë‚Çì‚ÇÅ‚Çá = mean(chains‚Çë‚Çì‚ÇÅ‚Çá[:V]);
move_up = V‚Çë‚Çì‚ÇÅ‚Çá / cos(Œ∏‚Çë‚Çì‚ÇÅ‚Çá)

plot_ellipses(N¬¥, x¬¥, y¬¥, Z¬¥, S¬¥)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚Çá*ùï© + b‚Çë‚Çì‚ÇÅ‚Çá, label="exr 17")
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚Çá*ùï© + b‚Çë‚Çì‚ÇÅ‚Çá + move_up, linestyle=:dash, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÅ‚Çá*ùï© + b‚Çë‚Çì‚ÇÅ‚Çá - move_up, linestyle=:dash, label=nothing)
plot!(ùï© -> m‚Çë‚Çì‚ÇÄ‚ÇÅ*ùï© + b‚Çë‚Çì‚ÇÄ‚ÇÅ, label="exr 01")
```

## exercise 18

::: {.callout-note}

Re-do exercise 17 but as a Bayesian, with sensible Bayesian priors on $(\theta,b_\perp,V)$. Find and marginalize the posterior distribution over $(\theta,b_\perp)$ to generate a marginalized posterior probability for the intrinsic variance parameter $V$. Plot this posterior with the 95% and 99% upper limits on $V$ marked. Why did we ask only for upper limits?

:::

```{julia}
# confidence level Œ± ‚Üí 2 quantiles: 1/2 ¬± Œ±/2
level9x = quantile(reduce(vcat, chains‚Çë‚Çì‚ÇÅ‚Çá[:V].data), [.5 + .95/2, .5 + .99/2])
density(chains‚Çë‚Çì‚ÇÅ‚Çá[:V], title="V", label=nothing)
vline!(level9x, label=["95%" "99%"])
```
